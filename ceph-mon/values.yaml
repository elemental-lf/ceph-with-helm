# Copyright 2017 The Openstack-Helm Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default values for ceph-mon.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

deployment:
  ceph: true
  storage_secrets: true
  client_secrets: false
  rbd_provisioner: true
  cephfs_provisioner: true

release_group: null

images:
  pull_policy: IfNotPresent
  tags:
    # See https://github.com/ceph/ceph-container for tags
    ceph_bootstrap:          'docker.io/ceph/daemon:v4.0.7-stable-4.0-nautilus-centos-7-x86_64'
    ceph_mon:                'docker.io/ceph/daemon:v4.0.7-stable-4.0-nautilus-centos-7-x86_64'
    ceph_osd:                'docker.io/ceph/daemon:v4.0.7-stable-4.0-nautilus-centos-7-x86_64'
    ceph_mgr:                'docker.io/ceph/daemon:v4.0.7-stable-4.0-nautilus-centos-7-x86_64'
    ceph_mds:                'docker.io/ceph/daemon:v4.0.7-stable-4.0-nautilus-centos-7-x86_64'
    ceph_rgw:                'docker.io/ceph/daemon:v4.0.7-stable-4.0-nautilus-centos-7-x86_64'
    #
    ceph_cephfs_provisioner: 'docker.io/elementalnet/cephfs-provisioner:0.3.0'
    ceph_rbd_provisioner:    'docker.io/elementalnet/rbd-provisioner:0.3.0'
    #
    dep_check: 'quay.io/airshipit/kubernetes-entrypoint:v1.0.0'
    ceph_config_helper: 'docker.io/elementalnet/ceph-config-helper:4.0.10-nautilus-k8s-1.17.3-rev-1'
    ceph_osd_operator: 'docker.io/elementalnet/ceph-osd-operator:0.2.0'
  local_registry:
    active: false
    exclude: []

labels:
  job:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  mon:
    node_selector_key: ceph-mon
    node_selector_value: enabled
  osd:
    node_selector_key: ceph-osd
    node_selector_value: enabled
  osd_operator:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  osd_maint:
    node_selector_key: ceph-osd-maint
    node_selector_value: enabled
  provisioner:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  mds:
    node_selector_key: ceph-mds
    node_selector_value: enabled
  mgr:
    node_selector_key: ceph-mgr
    node_selector_value: enabled
  rgw:
    node_selector_key: ceph-rgw
    node_selector_value: enabled

tolerations:
  mon: []
  mgr: []
  osd_operator: []
  provisioner: []

pod:
  security_context:
    # ceph-mon
    mon:
      pod:
        runAsUser: 65534
      container:
        init_dirs:
          runAsUser: 0
          readOnlyRootFilesystem: true
        mon:
          runAsUser: 0
          readOnlyRootFilesystem: true
    moncheck:
      pod:
        runAsUser: 65534
      container:
        moncheck:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    bootstrap:
      pod:
        runAsUser: 65534
      container:
        bootstrap:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    storage_keys_generator:
      pod:
        runAsUser: 65534
      container:
        storage_keys_generator:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    keyring_generator:
      pod:
        runAsUser: 65534
      container:
        keyring_generator:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true

    # ceph-osd
    osd:
      pod:
        runAsUser: 65534
      container:
        init_dirs:
          runAsUser: 0
          readOnlyRootFilesystem: true
        init:
          runAsUser: 0
          privileged: true
          readOnlyRootFilesystem: true
        osd:
          runAsUser: 0
          privileged: true
          readOnlyRootFilesystem: true
    osd_maint:
      pod:
        # Needs to be set to root so that kubectl exec processes are started as root.
        runAsUser: 0
      container:
        maint:
          runAsUser: 65534
          readOnlyRootFilesystem: true
    osd_operator:
      pod:
        # This is the user ansible-operator inside of the container.
        runAsUser: 1001
      container:
        ansible:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
        operator:
          allowPrivilegeEscalation: false
          # The image writes to /etc/passwd :(
          # See https://docs.openshift.com/container-platform/3.11/creating_images/guidelines.html#openshift-specific-guidelines
          readOnlyRootFilesystem: false

    # ceph-client
    mds:
      pod:
        runAsUser: 65534
      container:
        init_dirs:
          runAsUser: 0
          readOnlyRootFilesystem: true
        mds:
          runAsUser: 0
          readOnlyRootFilesystem: true
    mgr:
      pod:
        runAsUser: 65534
      container:
        init_dirs:
          runAsUser: 0
          readOnlyRootFilesystem: true
        mgr:
          runAsUser: 0
          readOnlyRootFilesystem: true
    mgr_init:
      pod:
        runAsUser: 65534
      container:
        mgr_init:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    rbd_pool:
      pod:
        runAsUser: 65534
      container:
        rbd_pool:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true

    # ceph-provisioner
    provisioner:
      pod:
        runAsUser: 65534
      container:
        cephfs_provisioner:
          runAsUser: 0
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
        rbd_provisioner:
          runAsUser: 0
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    namespace_client_key_generator:
      pod:
        runAsUser: 65534
      container:
        namespace_client_key_generator:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    namespace_client_key_cleaner:
      pod:
        runAsUser: 65534
      container:
        namespace_client_key_cleaner:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true

    # ceph-rgw
    rgw:
      pod:
        runAsUser: 65534
      container:
        init_dirs:
          runAsUser: 0
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
        rgw:
          runAsUser: 0
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
    rgw_s3_admin:
      pod:
        runAsUser: 65534
      container:
        s3_admin:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
  #
  dns_policy: "ClusterFirstWithHostNet"
  affinity:
    anti:
      type:
        default: preferredDuringSchedulingIgnoredDuringExecution
      topologyKey:
        default: kubernetes.io/hostname
  replicas:
    mon_check: 1
    mds: 2
    mgr: 2
    cephfs_provisioner: 2
    rbd_provisioner: 2
    rgw: 2
  lifecycle:
    upgrades:
      deployments:
        mds:
          pod_replacement_strategy: RollingUpdate
          revision_history: 3
          min_ready_seconds: 0
          rolling_update:
            max_surge: 1
            max_unavailable: 1
        mgr:
          pod_replacement_strategy: RollingUpdate
          revision_history: 3
          min_ready_seconds: 0
          rolling_update:
            max_surge: 1
            max_unavailable: 1
        moncheck:
          pod_replacement_strategy: Recreate
          revision_history: 3
          min_ready_seconds: 0
        rgw:
          pod_replacement_strategy: RollingUpdate
          revision_history: 3
          min_ready_seconds: 0
          rolling_update:
            max_surge: 1
            max_unavailable: 1
        cephfs_provisioner:
          pod_replacement_strategy: Recreate
          revision_history: 3
          min_ready_seconds: 0
        rbd_provisioner:
          pod_replacement_strategy: Recreate
          revision_history: 3
          min_ready_seconds: 0
      daemonsets:
        mon:
          pod_replacement_strategy: RollingUpdate
          revision_history: 3
          min_ready_seconds: 0
          rolling_update:
            max_unavailable: 1
  resources:
    enabled: false
    mon:
      requests:
        memory: "50Mi"
        cpu: "250m"
      limits:
        memory: "100Mi"
        cpu: "500m"
    mon_check:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    osd:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "1024Mi"
        cpu: "1000m"
    osd_maint:
      requests:
        memory: "512Mi"
        cpu: "500m"
      limits:
        memory: "1024Mi"
        cpu: "1000m"
    osd_operator_ansible:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    osd_operator_operator:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    mds:
      requests:
        memory: "10Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    mgr:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    rbd_provisioner:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    cephfs_provisioner:
      requests:
        memory: "5Mi"
        cpu: "250m"
      limits:
        memory: "50Mi"
        cpu: "500m"
    rgw:
      requests:
        memory: "128Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"
        cpu: "1000m"
    jobs:
      bootstrap:
        limits:
          memory: "1024Mi"
          cpu: "2000m"
        requests:
          memory: "128Mi"
          cpu: "500m"
      secret_provisioning:
        limits:
          memory: "1024Mi"
          cpu: "2000m"
        requests:
          memory: "128Mi"
          cpu: "500m"
      rgw_s3_admin:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"

secrets:
  keyrings:
    admin: ceph-client-admin-keyring
    mon: ceph-mon-keyring
    mds: ceph-bootstrap-mds-keyring
    osd: ceph-bootstrap-osd-keyring
    rgw: ceph-bootstrap-rgw-keyring
    mgr: ceph-bootstrap-mgr-keyring
    rbd: ceph-bootstrap-rbd-keyring
  rgw_s3:
    admin: radosgw-s3-admin-creds
  # ceph-rgw
  tls:
    ceph_object_store:
      api:
        public: ceph-tls-public
    ceph_mgr:
      mgr_dashboard:
        public: ceph-tls-mgr-dashboard

network:
  public: 192.168.0.0/16
  cluster: 192.168.0.0/16
  # ceph-rgw
  api:
    ingress:
      public: false
      classes:
        namespace: "nginx"
        cluster: "nginx"
      annotations:
        nginx.ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/proxy-max-temp-file-size: "0"
    external_policy_local: false
    node_port:
      enabled: false
      port: 30004
  mgr_dashboard:
    ingress:
      public: false
      classes:
        namespace: "nginx"
        cluster: "nginx"
      annotations:
        # A standby MGR will return 503 with 14.2.5 (and above) and our configuration.
        nginx.ingress.kubernetes.io/proxy-next-upstream: "error timeout http_503"
    external_policy_local: false
    node_port:
      enabled: false
      port: 30005

conf:
  features:
    mds: true
    mgr: true
    rgw: true
  templates:
    keyring:
      admin: |
        [client.admin]
          key = {{ key }}
          auid = 0
          caps mds = "allow"
          caps mon = "allow *"
          caps osd = "allow *"
          caps mgr = "allow *"
      mon: |
        [mon.]
          key = {{ key }}
          caps mon = "allow *"
      bootstrap:
        mds: |
          [client.bootstrap-mds]
            key = {{ key }}
            caps mon = "allow profile bootstrap-mds"
        mgr: |
          [client.bootstrap-mgr]
            key = {{ key }}
            caps mgr = "allow profile bootstrap-mgr"
        osd: |
          [client.bootstrap-osd]
            key = {{ key }}
            caps mon = "allow profile bootstrap-osd"
        rgw: |
          [client.bootstrap-rgw]
            key = {{ key }}
            caps mon = "allow profile bootstrap-rgw"
        rbd: |
          [client.bootstrap-rbd]
            key = {{ key }}
            caps mon = "allow profile bootstrap-rbd"
  ceph:
    global:
      cephx_cluster_require_signatures: true
      # This disables a warning which is generated for OSDs which were created before Nautilus.
      # Some statistics related to BlueStore metadata and data usage will probably be wrong.
      bluestore_warn_on_legacy_statfs: false
      osd_pool_default_pg_autoscale_mode: "off"
      device_failure_prediction_mode: local
    osd:
      ms_bind_port_min: 6800
      ms_bind_port_max: 7100
    mds:
      ms_bind_port_min: 7101
      ms_bind_port_max: 7300
  cephfs_provisioner:
    use_pvc_namespace_for_secrets: true
  rbd_provisioner:
    # Especially rbd rm might take longer for larger images.
    command_timeout: 600 # in seconds
  storage:
    mon:
      directory: /var/lib/openstack-helm/ceph/mon
    osd: []
#      - hosts: 
#          - cephosd041.example.com
#          - cephosd042.example.com
#          - cephosd043.example.com
#        osds: 
#          - data: '/dev/sdb'
#            db: '/dev/sdh1'
#            zap: false
#          - data: '/dev/sdc'
#            db: '/dev/sdh2'
#            zap: false
  pool:
  #NOTE(portdirect): this drives a simple approximation of
  # https://ceph.com/pgcalc/, the `target.osd` key should be set to match the
  # expected number of osds in a cluster, and the `target.pg_per_osd` should be
  # set to match the desired number of placement groups on each OSD.
    crush:
      #NOTE(portdirect): to use RBD devices with Ubuntu 16.04's 4.4.x series
      # kernel this should be set to `hammer`
      tunables: null
    target:
      #NOTE(portdirect): arbitrarily we set the default number of expected OSD's to 5
      # to match the number of nodes in the OSH gate.
      osd: 5
      pg_per_osd: 100
    default:
      #NOTE(portdirect): this should be 'same_host' for a single node
      # cluster to be in a healthy state
      #NOTE(lf): Only used for replicated pools, erasure coded pools
      # automatically generate a rule matching the selected profile
      crush_rule: replicated_rule
    #NOTE(portdirect): this section describes the pools that will be managed by
    # the ceph pool management job, as it tunes the pgs and crush rule, based on
    # the above.
    #NOTE(lf): For erasure coded pools the `replication` key should be set to k+m
    # to get the pg calculation right.
    spec:
      # RBD pool
      - name: rbd
        application: rbd
        replication: 3
        percent_total_data: 40
      # CephFS pools
      - name: cephfs_metadata
        application: cephfs
        replication: 3
        percent_total_data: 5
      - name: cephfs_data
        application: cephfs
        replication: 3
        percent_total_data: 10
      # RadosGW pools
      - name: .rgw.root
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.control
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.data.root
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.gc
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.log
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.intent-log
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.meta
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.usage
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.users.keys
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.users.email
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.users.swift
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.users.uid
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.buckets.extra
        application: rgw
        replication: 3
        percent_total_data: 0.1
      - name: default.rgw.buckets.index
        application: rgw
        replication: 3
        percent_total_data: 3
      - name: default.rgw.buckets.data
        application: rgw
        replication: 3
        percent_total_data: 34.8
  rgw_s3:
    enabled: false
    auth:
      admin:
        # NOTE(srwilkers): These defaults should be used for testing only, and
        # should be changed before deploying to production
        username: s3_admin
        access_key: "32AGKHCIG3FZ62IY1MEC"
        secret_key: "22S9iCLHcHId9AzAQD32O8jrq7DpFX9RHIOOC4NL"
        caps: "users=*;buckets=*;zone=*"
  storageclasses:
    # TODO: While it is possible to define further storage classes here, the creation of RBD client keys is still
    # TODO: hardcoded to the storage class name "rbd" and is not done for other RBD backed storage classes.
    # Two provisioners are supported: "ceph.com/rbd" and "ceph.com/cephfs".
    rbd:
      provision_storage_class: true
      ceph_configmap_name: ceph-etc
      default_storage_class: true
      provisioner: ceph.com/rbd
      allow_volume_expansion: true
      parameters:
        pool: rbd
        adminId: admin
        adminSecretName: pvc-ceph-conf-combined-storageclass
        adminSecretNamespace: ceph
        userId: admin
        userSecretName: pvc-ceph-client-key
        imageFormat: "2"
        # images_features is a comma separated list. Other features like
        #  - exclusive-lock
        #  - object-map
        #  - fast-diff
        #  - journal
        # only work with rbd-nbd and not with the default krbd client.
        imageFeatures: layering
    cephfs:
      provision_storage_class: true
      provisioner: ceph.com/cephfs
      metadata:
        name: cephfs
      parameters:
        adminId: admin
        adminSecretName: pvc-ceph-conf-combined-storageclass
        adminSecretNamespace: ceph
        #user_secret_name: pvc-ceph-cephfs-client-key
  mgr:
    # For a list of available modules: http://docs.ceph.com/docs/master/mgr/
    # Any module not listed here will be disabled.
    modules:
      - status
      - prometheus
      - balancer
      - iostat
      - rbd_support
      - dashboard
      - diskprediction_local
    # You can configure your mgr modules below. Each module has its own set of key/value. Refer to the doc
    # above for more info.
    config:
      #  Don't setup web service configuration options for the dashboard here.
      balancer:
        active: 1
        mode: crush-compat
      devicehealth:
        enable_monitoring: true
        self_heal: false
    dashboard:
      features:
        - cephfs
        - iscsi
        - mirroring
        - rbd
        - rgw
      users:
        - username: admin
          password: secret
          role: administrator
        - username: ro
          password: secret
          role: read-only

dependencies:
  static:
    bootstrap:
      jobs: null
      services:
        - endpoint: internal
          service: ceph_mon
    job_keyring_generator:
      jobs: null
    mon:
      jobs:
        - ceph-storage-keys-generator
        - ceph-mon-keyring-generator
    moncheck:
      jobs:
        - ceph-storage-keys-generator
        - ceph-mon-keyring-generator
      services:
        - endpoint: discovery
          service: ceph_mon
    osd:
      jobs:
        - ceph-storage-keys-generator
        - ceph-osd-keyring-generator
      services:
        - endpoint: internal
          service: ceph_mon
    storage_keys_generator:
      jobs: null
    cephfs_client_key_generator:
      jobs: null
    cephfs_provisioner:
      jobs:
        - labels:
            application: ceph
            component: rbd-pool
      services:
        - endpoint: internal
          service: ceph_mon
    mds:
      jobs:
        - name: ceph-storage-keys-generator
        - name: ceph-mds-keyring-generator
        - labels:
            application: ceph
            component: rbd-pool
      services:
        - endpoint: internal
          service: ceph_mon
    mgr:
      jobs:
        - ceph-storage-keys-generator
        - ceph-mgr-keyring-generator
      services:
        - endpoint: internal
          service: ceph_mon
    namespace_client_key_cleaner:
      jobs: null
    namespace_client_key_generator:
      jobs: null
    rbd_pool:
      services:
        - endpoint: internal
          service: ceph_mon
    mgr_init:
      services:
        - endpoint: internal
          service: ceph_mgr
    rbd_provisioner:
      jobs:
        - labels:
            application: ceph
            component: rbd-pool
      services:
        - endpoint: internal
          service: ceph_mon
    rgw_s3_admin:
      services:
        - endpoint: internal
          service: ceph_object_store

endpoints:
  cluster_domain_suffix: cluster.local
  ceph_mon:
    namespace: null
    hosts:
      default: ceph-mon
      discovery: ceph-mon-discovery
    host_fqdn_override:
      default: null
    port:
      mon:
        # Don't change these ports. They need to be set to the official values, so that automatic discovery of the
        # messaging protocol works.
        default: 6789
        v2: 3300
  ceph_mgr:
    namespace: null
    hosts:
      default: ceph-mgr
      public: ceph-mgr
    host_fqdn_override:
      default: null
    port:
      mgr:
        default: 7000
      metrics:
        default: 9283
      dashboard:
        default: 8080
    scheme:
      default: http
  ceph_object_store:
    name: ceph-rgw
    namespace: null
    hosts:
      default: ceph-rgw
      public: ceph-rgw
    host_fqdn_override:
      default: null
      # NOTE(portdirect): this chart supports TLS for fqdn over-ridden public
      # endpoints using the following format:
      # public:
      #   host: null
      #   tls:
      #     crt: null
      #     key: null
    path:
      default: null
    scheme:
      default: http
    port:
      api:
        default: 8088
        public: 80
        
monitoring:
  prometheus:
    enabled: true
    ceph_mgr:
      scrape: true
      port: 9283

manifests:
  # ceph-mon
  configmap_mon_bin: true
  configmap_etc: true
  configmap_templates: true
  daemonset_mon: true
  deployment_moncheck: true
  job_bootstrap: true
  job_keyring: true
  service_mon: true
  service_mon_discovery: true
  job_storage_admin_keys: true
  # ceph-osd
  configmap_osd_bin: true
  deployment_osd_operator: true
  cr_osds: true
  daemonset_osd_maint: true
  # ceph-client
  configmap_client_bin: true
  deployment_mds: true
  deployment_mgr: true
  job_cephfs_client_key: true
  job_rbd_pool: true
  job_mgr_init: true
  service_mgr: true
  ingress_mgr_dashboard: true
  secret_ingress_tls_mgr_dashboard: true
  # ceph-provisioners
  configmap_provisioners_bin: true
  deployment_rbd_provisioner: true
  deployment_cephfs_provisioner: true
  job_namespace_rbd_client_key_cleaner: true
  job_namespace_rbd_client_key: true
  storageclasses: true
  # ceph-rgw
  configmap_rgw_bin: true
  deployment_rgw: true
  ingress_rgw: true
  job_s3_admin: true
  secret_s3_rgw: true
  secret_ingress_tls: true
  service_ingress_rgw: true
  service_rgw: true
